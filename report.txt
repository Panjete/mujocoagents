Team Assignment by Gurarmaan Singh Panjeta (2020CS50426) and Aryan Dua (2020CS50475)

Key Insights and Implementational details of Imitation Agent:

> NN Model : Simple MLP , Learning Rate = 1e-3,
> Beta (DAGGER Constant) : Decreases as itr_num increases ~ (1/1+(itr_num))
> With Probability Beta, train on Expert Policy, with 1-Beta, train on your action
> After every iteration, if model performs better than previous best, checkpoint it!
> For every training iteration, randomly sample 10 trajectories from the buffer and retrain on them. 
> Best Performance : Hopper <2353>, HalfCheetah <2966>, Ant <1396>

Key Insights and Implementational details of Imitation Agent:

> Vanilla Reinforce is just too unstable
> With Baseline, doesn't seem to train on anything at all
> With Actor Critic, learns and saturates for small patches on a small reward (~200). Does not explore enough to 
> start hopping though

> Implementation of Soft Actor Critic : Theoretic results from [https://arxiv.org/abs/1801.01290] suggest 
> taking minimum of multiple critics to combat over estimation, soft-transition of parameters (tau) 
> representation (reward_now * gamma * Q(s,a) - V(s)) of Advantage, and maximizing "entropy"
> Declaration : Implementation With help from [https://www.youtube.com/watch?v=ioidsRlf79o] 


Key Insights and Implementational Details from ImitationSeededRL:

> In the first Iteration of training, we train the Imitation attribute ("the imitator") to fit the expert policy well
> Once trained, we fit the actor model to actions suggest by the imitaton on sampled trajectories, and the critic to 
> the "rewards-to-go" obtained from these trajectories on a per-state basis.

> The ImitationSeededRL version of the Soft Actor Critic is purely our contribution

> On subsequent iterations, with probability p, perform some training iterations on the imitator and 
> "recalibrate" the actor and critic to the imitator
> With probability 1-p, let the RL component of the model explore
> As the number of iterations increase, decrease p (explore more).

Overall Optimisations:

> Greedily save the model when a test set performs better than the previous best performance

> We have also attached (working code) of the vanilla actor-critic and baseline versions of the RL agent 
> If needed, these can be run by simply uncommenting them and commenting SAC's RL and ImitationSeeded versions
> Comments in the file have been placed to make this switch convenient

> Analysis of performance has has been meticulously performed on the trainings. Some Plots of performance vs training 
> have been attached 


