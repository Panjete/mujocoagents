Team Assignment with equal contributions by Gurarmaan Singh Panjeta (2020CS50426) and Aryan Dua (2020CS50475)

Key Insights and Implementational details of Imitation Agent:

> NN Model : Simple MLP , Learning Rate = 1e-3,
> Beta (DAGGER Constant) : Decreases as itr_num increases ~ (1/1+(itr_num))
> With Probability Beta, train on Expert Policy, with 1-Beta, train on your action
> After every iteration, if model performs better than previous best, checkpoint it!
> For every training iteration, randomly sample 10 trajectories from the buffer and retrain on them. 
> Best Performance : Hopper <2353>, HalfCheetah <2966>, Ant <1396>

Key Insights and Implementational details of Reinforcement Learning Agent:

> Continuous Action space is handled by assuming that all dimensions of actions are independent of each other, 
> and using a (not too unrealistic) assumption that the actions can be assumed to be coming from a Normal Distribution
> Thus, the model outputs 2*action_dim number of values, half being the means and the other half being variances
> When querying for the next action, a sample from this distribution is returned

> Vanilla Reinforce is just too unstable
> With Baseline, doesn't seem to train on anything at all
> With Actor Critic, learns and (unstably) stays for small patches on a small reward (~1000).
> Does not explore enough to start hopping though
> Every training iteration, a random sample of 10 trajectories from the replay buffer is re-trained to combat forgetfulness

> We also incentivise higher traj lengths : by injecting reward proportional to remaining length of trajectory for that state
> The rationale behing this is that for the given (s), the model could run for (T-t) more timesteps, and if T-t is higher, the mode is able to run more

> Gradients that are added to the current policy are weighed by alpha :  sqrt(1/(1 + envsteps_so_far/1000))
> This decreases as iterations increase, inspiration from simulated-annealing.

> Implementation of Soft Actor Critic : Theoretic results from [https://arxiv.org/abs/1801.01290] suggest 
> taking minimum of multiple critics to combat over estimation, soft-transition of parameters (tau) 
> representation (reward_now * gamma * Q(s,a) - V(s)) of Advantage, and maximizing "entropy"
> Declaration : Implementation With help from [https://www.youtube.com/watch?v=ioidsRlf79o] 
> Soft-Updates of parameters for stability, multi-critics to combat over-estimation
> 


Key Insights and Implementational Details from ImitationSeededRL:

> In the first Iteration of training, we train the Imitation attribute ("the imitator") to fit the expert policy well
> Once trained, we fit the actor model to actions suggest by the imitaton on sampled trajectories, and the critic to 
> the "rewards-to-go" obtained from these trajectories on a per-state basis.

> The ImitationSeededRL version of the Soft Actor Critic is purely our contribution

> On subsequent iterations, with probability p, perform some training iterations on the imitator and 
> "recalibrate" the actor and critic to the imitator
> With probability 1-p, let the RL component of the model explore
> As the number of iterations increase, decrease p (explore more).

Overall Optimisations:

> Greedily save the model when a test set performs better than the previous best performance

> We have also attached (working code) of the vanilla actor-critic and baseline versions of the RL agent 
> If needed, these can be run by simply uncommenting them and commenting SAC's RL and ImitationSeeded versions
> Comments in the file have been placed to make this switch convenient

> Analysis of performance has has been meticulously performed on the trainings. Some Plots of performance vs training 
> have been attached in [https://drive.google.com/drive/folders/1PnE2SqnoBilEAzfqQvOopdSEPeQdkDtW?usp=sharing]
> This was done by modifying the train_agent.py, and plotting values from the evaluated scalar logs



